{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "\n",
    "#importing all the necessary lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cranfield_docs_url = \"/kaggle/input/cransfield/cran_docs.json\"\n",
    "cranfield_queries_url = \"/kaggle/input/cransfield/cran_queries.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_corpus(url,mode):\n",
    "  \n",
    "    \"\"\"\n",
    "    Given a JSON file containing a list of dictionaries with at least one key being mode,\n",
    "    preprocesses and tokenizes the values associated with mode.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): URL of the JSON file\n",
    "    mode (str): key of the dictionary in the JSON file to preprocess and tokenize\n",
    "\n",
    "    Returns:\n",
    "    docs (np.array): array of preprocessed and tokenized documents\n",
    "    types (set): set of unique tokens found in the documents\n",
    "    \"\"\"\n",
    "    \n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    with open(url, 'r') as f: data = json.load(f)\n",
    "\n",
    "    num_docs = len(data)\n",
    "    docs = np.empty(num_docs, dtype='object')\n",
    "\n",
    "    for i in range(num_docs): docs[i] = data[i][mode]\n",
    "\n",
    "    # Preprocessing\n",
    "    docs = [[[word for word in list(TextBlob(doc).words) if word not in stop_words] for doc in sent_detector.tokenize(d.strip())] for d in docs]\n",
    "\n",
    "    # Appending sentences to a single token document\n",
    "    merged_doc = []\n",
    "    types = set()\n",
    "\n",
    "    # Merge all sentences in one single token document\n",
    "    for doc in docs:\n",
    "        temp_doc = []\n",
    "        for sentence in doc:\n",
    "            temp_doc+=sentence\n",
    "            for word in sentence:\n",
    "                types.add(word)\n",
    "        merged_doc.append(temp_doc)\n",
    "\n",
    "    docs = merged_doc\n",
    "    return docs,types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs,types_docs = url_to_corpus(cranfield_docs_url,'body')\n",
    "queries,types_queries = url_to_corpus(cranfield_queries_url,'query')\n",
    "\n",
    "types = list(types_docs.union(types_queries))\n",
    "#here types is union of two sets, \n",
    "#datatype of types is list -> although it is formed by union of two sets\n",
    "#union operations here only considers the unique elements(not duplicated elements)\n",
    "#finally (types) have have all the unique strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 0\n",
    "\n",
    "\n",
    "for seq in docs:\n",
    "    if len(seq) > max_seq_length : max_seq_length = len(seq)\n",
    "\n",
    "for seq in queries:\n",
    "    if len(seq) > max_seq_length : max_seq_length = len(seq)\n",
    "\n",
    "#these two are added because we need to consider start and end that will be added in next step\n",
    "max_seq_length += 2\n",
    "print(max_seq_length)\n",
    "#max_seq_length holds the the maximum size of the sequence that we encounter in docs and queries\n",
    "#this is used to pad the sequences which are not similar to max_seq\n",
    "\n",
    "def types_to_idx(types):\n",
    "    #what this function d\n",
    "    seq_idx = {}\n",
    "\n",
    "    for t in types : seq_idx[t] = len(seq_idx)\n",
    "\n",
    "    return seq_idx\n",
    "\n",
    "seq_idx = types_to_idx(types)\n",
    "#seq_idx a dict which maps the tokens to a number\n",
    "#and below we added four special tokens\n",
    "seq_idx['/start'] = len(seq_idx)\n",
    "seq_idx['/end'] = len(seq_idx)\n",
    "seq_idx['/unknown'] = len(seq_idx)\n",
    "seq_idx['/pad'] = len(seq_idx)\n",
    "\n",
    "#start,end,unknown,pad are mapped to 9235 9236 9237 9238 resply\n",
    "print(seq_idx)\n",
    "\n",
    "def doc_to_seq(docs, seq_idx, max_seq_length, mode):\n",
    "\n",
    "    seqs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "\n",
    "        seq = []\n",
    "        seq += doc\n",
    "        \n",
    "        if(mode=='pad') : \n",
    "            \n",
    "            print(\"Done\")\n",
    "            seq.insert(0,'/start')\n",
    "            seq.append('/end')\n",
    "            while(len(seq)<max_seq_length) : seq.insert(-1,'/pad')\n",
    "            #-1 inserts the element at last-but-one index\n",
    "\n",
    "        seq = [seq_idx[word] for word in seq]\n",
    "        seqs.append(seq)\n",
    "\n",
    "    #seqs will consist of all the integers, this integers are mapping of the tokens to the id(number)    \n",
    "    return seqs\n",
    "\n",
    "#do padding stuff and then convert the list into numpy array\n",
    "doc_seq = np.array(doc_to_seq(docs, seq_idx, max_seq_length, 'pad'))\n",
    "query_seq = np.array(doc_to_seq(queries, seq_idx, max_seq_length, 'pad'))\n",
    "\n",
    "print(doc_seq.shape)\n",
    "print(query_seq.shape)\n",
    "\n",
    "np.savetxt('cranfield_sequences/doc_seq.csv', doc_seq, fmt='%s')\n",
    "np.savetxt('cranfield_sequences/q_seq.csv', query_seq, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "\n",
    "with open(\"/kaggle/input/cransfield/cran_qrels.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "doc_seq = np.genfromtxt('/kaggle/input/cransfield/doc_seq.csv', dtype=int)\n",
    "#these are not embeddings\n",
    "\n",
    "query_seq = np.genfromtxt('/kaggle/input/cransfield/q_seq.csv', dtype=int)\n",
    "y = tf.one_hot(np.array([data[i]['position'] for i in range(len(data))]),depth=4)\n",
    "x_doc_seq = np.array([doc_seq[int(data[i]['id'])-1] for i in range(len(data))])\n",
    "x_query_seq = np.array([query_seq[int(data[i]['query_num'])-1] for i in range(len(data))])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
